{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjsyqrUcD20"
      },
      "source": [
        "# Installing reqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpXp3jXS6K4n"
      },
      "outputs": [],
      "source": [
        "!pip install googletransx\n",
        "!pip install aiohttp==3.7.0\n",
        "!pip install emoji\n",
        "!pip install pytrends\n",
        "!pip install pandas==0.25.1\n",
        "\n",
        "!git clone --depth=1 https://github.com/twintproject/twint.git\n",
        "%cd twint\n",
        "!pip3 install . -r requirements.txt\n",
        "!python setup.py install\n",
        "%cd ..\n",
        "\n",
        "!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGBQOERsthIl",
        "outputId": "1da14332-4bd7-4a3b-e9a7-197c0dfd1c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: nltk\n",
            "Version: 3.2.5\n",
            "Summary: Natural Language Toolkit\n",
            "Home-page: http://nltk.org/\n",
            "Author: Steven Bird\n",
            "Author-email: stevenbird1@gmail.com\n",
            "License: Apache License, Version 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: six\n",
            "Required-by: textblob\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"textblob.zip\" \"/usr/local/lib/python3.7/dist-packages/textblob\"\n",
        "!zip -r \"nltk.zip\" \"/usr/local/lib/python3.7/dist-packages/nltk\""
      ],
      "metadata": {
        "id": "dru6riMeUNWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GThRlJxO9P6"
      },
      "outputs": [],
      "source": [
        "# import reqs\n",
        "import nest_asyncio\n",
        "import googletransx\n",
        "import twint \n",
        "import pandas as pd\n",
        "import os, sys\n",
        "#import findspark\n",
        "from textblob import TextBlob\n",
        "#from pyspark.sql.functions import udf\n",
        "import seaborn as sns\n",
        "import re\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import json\n",
        "from pytrends.request import TrendReq\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "from pandas.core.algorithms import unique\n",
        "\n",
        "# setting env paths for optimus reqs\n",
        "#findspark.init(spark_home=\"/content/spark-2.4.1-bin-hadoop2.7\")\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\"\n",
        "\n",
        "# pd for not printing warning in console\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# avoiding some errors with event loops\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81YIJWRzb0ue"
      },
      "source": [
        "# Scrapping tweets using Twint (no API needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u9n0GssTM1Z"
      },
      "outputs": [],
      "source": [
        "# some useful functions for twint\n",
        "def available_columns():\n",
        "    return twint.output.panda.Tweets_df.columns\n",
        "\n",
        "def twint_to_pandas(columns):\n",
        "    return twint.output.panda.Tweets_df[columns]\n",
        "\n",
        "# Class for disabling printings in console\n",
        "class HiddenPrints:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsXR_6qwYV07"
      },
      "outputs": [],
      "source": [
        "# function to get sentiment \n",
        "def apply_blob(sentence):\n",
        "    temp = TextBlob(sentence).sentiment\n",
        "    sent = 0       # Neutral\n",
        "    if temp[0] >= 0.25:\n",
        "        sent = 1.0 # Positive\n",
        "    elif temp[0] <= -0.25:\n",
        "        sent = -1.0 # Negative\n",
        "    return sent, temp[0], temp[1]\n",
        "\n",
        "# tweets processing + getting sentiments for each tweet\n",
        "def tweets_sentiment(df_pd):\n",
        "    if df_pd[\"tweet\"].count() != 0:\n",
        "        # Clean tweets    \n",
        "        for i in range(len(df_pd[\"tweet\"])):\n",
        "            temp = df_pd[\"tweet\"].iloc[i]\n",
        "            # remove links\n",
        "            temp = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]\n",
        "                          |[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()\n",
        "                          <>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\n",
        "                          \\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\))\n",
        "                          )*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", temp)\n",
        "            # remove unwanted chars\n",
        "            temp = re.sub('[@$%0123456789:/.:;=*]', '', temp)\n",
        "            # convert emoji into words\n",
        "            temp = emoji.demojize(temp, delimiters=(\"\",\" \"))\n",
        "            # remove multiple spaces\n",
        "            temp = re.sub(' +', ' ', temp)\n",
        "            # remove _ \n",
        "            temp = re.sub('_', ' ', temp)\n",
        "            # saving changes to df\n",
        "            df_pd[\"tweet\"].iloc[i] = temp\n",
        "        \n",
        "        # adding sentiments feature to df_pd\n",
        "        sentiment_round = []\n",
        "        sentiment = []\n",
        "        objectivity = []\n",
        "        sent_obj_prod = []\n",
        "        for x in df_pd[\"tweet\"]:\n",
        "            sent_round, sent, subj = apply_blob(x)\n",
        "            sentiment_round.append(sent_round)\n",
        "            sentiment.append(sent)\n",
        "            objectivity.append(1 - subj)\n",
        "            sent_obj_prod.append(sent * (1 - subj))\n",
        "\n",
        "        # Add sentiment to return df\n",
        "        df_pd[\"sentiment\"] = sentiment\n",
        "        df_pd[\"sentiment_round\"] = sentiment_round\n",
        "        df_pd[\"objectivity\"] = objectivity\n",
        "        df_pd[\"sent_obj_prod\"] = sent_obj_prod\n",
        "\n",
        "    return df_pd\n",
        "\n",
        "\n",
        "def tweets_collect(keyword, limit=1, min_likes=1000, min_replies=500, \n",
        "                   min_retweets=50, popular_tweets=True, hide_output=True, \n",
        "                   since=None, until=None):\n",
        "    # creating object for search config\n",
        "    c = twint.Config()\n",
        "\n",
        "    # search tweets of specific user\n",
        "    #c.Username = \"narendramodi\"\n",
        "\n",
        "    # specifying tweets language\n",
        "    c.Lang = \"en\"\n",
        "\n",
        "    # entering keywords to search for in tweets\n",
        "    c.Search = keyword\n",
        "\n",
        "    # number of tweets to search\n",
        "    c.Limit = limit\n",
        "\n",
        "    # adding conditions with interation tools (Like, Retweet, Reply)\n",
        "    c.Min_likes = min_likes\n",
        "    c.Min_replies = min_replies\n",
        "    c.Min_retweets = min_retweets\n",
        "\n",
        "    # get popular / recent tweets\n",
        "    c.Popular_tweets = popular_tweets\n",
        "\n",
        "    # adding date period for tweets searching (format: '2016-05-02 00:00:00')\n",
        "    if since != None:\n",
        "        c.Since = since\n",
        "    if until != None:\n",
        "        c.Until = until\n",
        "    #c.Until= '2018-05-10 23:59:59'\n",
        "\n",
        "    # no writing in console\n",
        "    c.Hide_output = hide_output\n",
        "\n",
        "    # something with pandas\n",
        "    c.Pandas = True\n",
        "    \n",
        "    # running scrapping\n",
        "    with HiddenPrints():\n",
        "        twint.run.Search(c)\n",
        "\n",
        "    # in case 0 tweets found\n",
        "    if len(available_columns()) == 0:\n",
        "        column_names = [\"date\", \"username\", \"tweet\", \"nlikes\", \"nreplies\", \n",
        "                        \"nretweets\", \"language\"]\n",
        "        df_pd = pd.DataFrame(columns = column_names, dtype=object)\n",
        "        return(df_pd)\n",
        "    \n",
        "    # Transform tweets to pandas DF\n",
        "    df_pd = twint_to_pandas([\"date\", \"username\", \"tweet\", \"nlikes\", \n",
        "                             \"nreplies\", \"nretweets\", \"language\"])\n",
        "    # Saving only English tweets\n",
        "    df_pd = df_pd[df_pd.language == \"en\"]\n",
        "\n",
        "    df_pd = df_pd.drop(['language'], axis=1)\n",
        "    \n",
        "    return df_pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjgJkmBs8r6J"
      },
      "source": [
        "Function for scraping tweets for some period and getting all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVF3dRmSo4_0"
      },
      "outputs": [],
      "source": [
        "def tweet_data_collect(keyword, since, until, limit = 100, min_likes=0, \n",
        "                       min_replies=0, min_retweets=0, popular_tweets=True, \n",
        "                       n_rescans = 5, n_retries=10):\n",
        "    \n",
        "    # dividing needed timeline into minutes to get more data\n",
        "    daterange = pd.date_range(since, until, freq='d').strftime('%Y-%m-%d')\n",
        "    date = [ str(x) for x in daterange]\n",
        "\n",
        "    # creating resulting dataset\n",
        "    column_names = [\"date\", \"username\", \"tweet\", \"nlikes\", \"nreplies\", \n",
        "                    \"nretweets\", \"language\"]\n",
        "    df_result = pd.DataFrame(columns = column_names, dtype=object)\n",
        "\n",
        "    # finding process for each minute\n",
        "    for hour_n in tqdm(range(len(daterange))):\n",
        "\n",
        "        since_cur = daterange[hour_n]+' 00:00:00'\n",
        "        until_cur = daterange[hour_n]+' 23:59:59'\n",
        "        df = pd.DataFrame()\n",
        "        tweets_cur_max = 0\n",
        "        \n",
        "        for rescan in range(n_rescans):\n",
        "            \n",
        "            temp_df = pd.DataFrame()\n",
        "\n",
        "            for attempt in range(n_retries+1):\n",
        "                try:  \n",
        "                    temp_df = tweets_collect(keyword = keyword, limit=limit, \n",
        "                                             min_likes=min_likes, \n",
        "                                             min_replies=min_replies, \n",
        "                                             min_retweets=min_retweets, \n",
        "                                             popular_tweets=popular_tweets, \n",
        "                                             since=since_cur, \n",
        "                                             until=until_cur)\n",
        "                except Exception:\n",
        "                    if attempt < n_retries:\n",
        "                        sleep_time = 2 * 2 ** attempt\n",
        "                        print(f'Waiting {sleep_time:.0f} seconds', flush=True)\n",
        "                        time.sleep(sleep_time)\n",
        "                    else:\n",
        "                        print('Blocked by twitter still')\n",
        "                        return 1\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            # if found more data than before -> save more data\n",
        "            n_found_tweets = temp_df['tweet'].count()\n",
        "            if n_found_tweets > tweets_cur_max:\n",
        "                df = temp_df\n",
        "                tweets_cur_max = n_found_tweets\n",
        "\n",
        "        # merging found data with result dataframe\n",
        "        df_result = pd.concat([df_result, df])\n",
        "    \n",
        "    return df_result\n",
        "\n",
        "\n",
        "def tweet_feature_collect(pd_df):\n",
        "\n",
        "    # total (sum) day interactivity params\n",
        "    day_likes = []\n",
        "    day_retweets = []\n",
        "    day_replies = []\n",
        "    # average day interactivity params\n",
        "    mean_likes = []\n",
        "    mean_retweets = []\n",
        "    mean_replies = []\n",
        "    # total day tweets num    \n",
        "    tweets_vol = []\n",
        "    # unique users num\n",
        "    unique_users_num = []\n",
        "    # metric from found paper\n",
        "    interactivity = []\n",
        "    # mean day sentiment score (both raw and rounded)\n",
        "    mean_sentiment = []\n",
        "    mean_sentiment_round = []\n",
        "    # sum of daily sentiment scores\n",
        "    sum_sentiment = []\n",
        "    # subjectivity metrics\n",
        "    mean_objectivity = []\n",
        "    mean_sent_obj_prod = []\n",
        "\n",
        "    daterange = pd.date_range(pd_df['date'].min(), pd_df['date'].max())\n",
        "    date = [ str(x.date()) for x in daterange]\n",
        "\n",
        "    for day_n in tqdm(range(len(daterange))):\n",
        "\n",
        "        since_cur = str(daterange[day_n].date())+' 00:00:00'\n",
        "        until_cur = str(daterange[day_n].date())+' 23:59:59'\n",
        "\n",
        "        df = pd_df[(pd_df['date'] > since_cur) & (pd_df['date'] < until_cur)]\n",
        "\n",
        "        if df['tweet'].count() == 0:\n",
        "            day_likes.append(0)\n",
        "            day_replies.append(0)\n",
        "            day_retweets.append(0)\n",
        "            mean_likes.append(0)\n",
        "            mean_replies.append(0)\n",
        "            mean_retweets.append(0)\n",
        "            tweets_vol.append(0)\n",
        "            unique_users_num.append(0)\n",
        "            interactivity.append(np.NAN)\n",
        "            mean_sentiment_round.append(np.NAN)\n",
        "            mean_sentiment.append(np.NAN)\n",
        "            sum_sentiment.append(np.NAN)\n",
        "            mean_objectivity.append(np.NAN)\n",
        "            mean_sent_obj_prod.append(np.NAN)\n",
        "            continue\n",
        "\n",
        "        day_likes.append(df['nlikes'].sum())\n",
        "        day_replies.append(df['nreplies'].sum())\n",
        "        day_retweets.append(df['nretweets'].sum())\n",
        "\n",
        "        mean_likes.append(round(df['nlikes'].mean()))\n",
        "        mean_replies.append(round(df['nreplies'].mean()))\n",
        "        mean_retweets.append(round(df['nretweets'].mean()))\n",
        "\n",
        "        tweets_vol.append(df['tweet'].count())\n",
        "        unique_users_num.append(len(df['username'].unique()))\n",
        "\n",
        "        interactivity.append((day_likes[day_n] + day_replies[day_n] + \n",
        "                              day_retweets[day_n]) / tweets_vol[day_n])\n",
        "        \n",
        "        mean_sentiment_round.append(df['sentiment_round'].mean())\n",
        "        mean_sentiment.append(df['sentiment'].mean())\n",
        "        sum_sentiment.append(df['sentiment'].sum())\n",
        "        \n",
        "        mean_objectivity.append(df['objectivity'].mean())\n",
        "        mean_sent_obj_prod.append(df['sent_obj_prod'].mean())\n",
        "\n",
        "    df_result = pd.DataFrame()\n",
        "\n",
        "    df_result['date'] = date\n",
        "    \n",
        "    df_result['day_likes'] = day_likes\n",
        "    df_result['day_replies'] = day_replies\n",
        "    df_result['day_retweets'] = day_retweets\n",
        "    \n",
        "    df_result['mean_likes'] = mean_likes\n",
        "    df_result['mean_replies'] = mean_replies\n",
        "    df_result['mean_retweets'] = mean_retweets\n",
        "    \n",
        "    df_result['tweets_vol'] = tweets_vol\n",
        "    df_result['unique_users_num'] = unique_users_num\n",
        "    df_result['interactivity'] = interactivity\n",
        "    \n",
        "    df_result['mean_sentiment_round'] = mean_sentiment_round\n",
        "    df_result['mean_sentiment'] = mean_sentiment\n",
        "    df_result['sum_sentiment'] = sum_sentiment\n",
        "    \n",
        "    df_result['mean_objectivity'] = mean_objectivity\n",
        "    df_result['mean_sent_obj_prod'] = mean_sent_obj_prod\n",
        "\n",
        "    return df_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L29Hk3JLlpMi"
      },
      "source": [
        "# Finally collecting tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXpnV1Qh6K7F",
        "outputId": "c5db4673-f377-4cb2-8973-07eb3f777482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/365 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting 2 seconds\n",
            "Waiting 4 seconds\n",
            "Waiting 8 seconds\n",
            "Waiting 16 seconds\n",
            "Waiting 32 seconds\n",
            "Waiting 64 seconds\n",
            "Waiting 128 seconds\n",
            "Waiting 256 seconds\n",
            "Waiting 512 seconds\n",
            "Waiting 1024 seconds\n",
            "Waiting 2 seconds\n",
            "Waiting 2 seconds\n",
            "Waiting 2 seconds\n",
            "Waiting 2 seconds\n",
            "Waiting 2 seconds\n",
            "Waiting 2 seconds\n",
            "Waiting 2 seconds\n",
            "Waiting 2 seconds\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n"
          ]
        }
      ],
      "source": [
        "keyword = 'flex coin'\n",
        "#since = \"2014-01-01\"\n",
        "#until = \"2022-05-24\"\n",
        "since = \"2021-01-01 00:00:00\"\n",
        "until = \"2021-12-31 23:59:59\"\n",
        "limit = 10000\n",
        "\n",
        "tweet_df = tweet_data_collect(keyword, since, until, limit = limit, min_likes=0, \n",
        "                              min_replies=0, min_retweets=0, popular_tweets=True)\n",
        "\n",
        "tweet_df.to_csv('tweets_flex_2021.csv')\n",
        "\n",
        "df_result = tweets_sentiment(tweet_df)\n",
        "\n",
        "df_result.to_csv('tweets_flex_sent_2021.csv')\n",
        "\n",
        "df_final = tweet_feature_collect(df_result)\n",
        "\n",
        "df_final.to_csv('tweets_flex_features_2021.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwYoyNkUl8-Z"
      },
      "source": [
        "Concatenating DF's for different years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b00_W4pfq-0P"
      },
      "outputs": [],
      "source": [
        "tweets_all = pd.concat([pd.read_csv('tweets_features_2014.csv', lineterminator='\\n'), \n",
        "                        pd.read_csv('tweets_features_2015.csv', lineterminator='\\n'),\n",
        "                        pd.read_csv('tweets_features_2016.csv', lineterminator='\\n'),\n",
        "                        pd.read_csv('tweets_features_2017.csv', lineterminator='\\n'),\n",
        "                        pd.read_csv('tweets_features_2018.csv', lineterminator='\\n'),\n",
        "                        pd.read_csv('tweets_features_2019.csv', lineterminator='\\n'),\n",
        "                        pd.read_csv('tweets_features_2020.csv', lineterminator='\\n'),\n",
        "                        pd.read_csv('tweets_features_2021.csv', lineterminator='\\n'),\n",
        "                        pd.read_csv('tweets_features_2022.csv', lineterminator='\\n')])\n",
        "\n",
        "tweets_all.to_csv('tweets_features_2014_2021.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoM2dXVTfFQw"
      },
      "source": [
        "Adding Fear / Greed market index features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm5EG6W9e_A5"
      },
      "outputs": [],
      "source": [
        "# Url for Crypto Fear & Greed Index:\n",
        "url = \"https://api.alternative.me/fng/?limit=0\"\n",
        "\n",
        "# Making a get request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Crypto fear and greed data into a dataframe:\n",
        "fg_crypto_df = pd.DataFrame(response.json()['data'])\n",
        "\n",
        "# Change timestamp values into dates:\n",
        "fg_crypto_df['timestamp'] = pd.to_datetime(fg_crypto_df['timestamp'], unit=\"s\")\n",
        "\n",
        "# Change column names:\n",
        "fg_crypto_df.columns = ['Value', 'Label', 'Date', 'Time Until Update']\n",
        "\n",
        "# Change value into a numeric column:\n",
        "fg_crypto_df['Value'] = pd.to_numeric(fg_crypto_df['Value'])\n",
        "\n",
        "fg_crypto_df = fg_crypto_df.drop(['Time Until Update'], axis = 1)\n",
        "\n",
        "fg_crypto_df = fg_crypto_df[fg_crypto_df.Date < pd.Timestamp('2022-05-14')]\n",
        "\n",
        "fg_labels = fg_crypto_df['Label'].tolist()\n",
        "fg_values = fg_crypto_df['Value'].tolist()\n",
        "\n",
        "extra_len = tweets_all['date'].count() - len(fg_values)\n",
        "\n",
        "nan_list = [np.NaN] * extra_len\n",
        "blank_list = [''] * extra_len\n",
        "\n",
        "fg_labels_extend = [ *fg_labels, *blank_list ]\n",
        "fg_values_extend = [ *fg_values, *nan_list ]\n",
        "\n",
        "fg_labels_extend.reverse()\n",
        "fg_values_extend.reverse()\n",
        "\n",
        "tweets_all['fear_greed_labels'] = fg_labels_extend\n",
        "tweets_all['fear_greed_values'] = fg_values_extend\n",
        "\n",
        "tweets_all.to_csv('tweets_features_2014_2022_fg.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "twitter_parse_light.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}